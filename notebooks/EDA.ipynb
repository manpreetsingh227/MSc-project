{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to load in tables of varying depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def filter_df(fname):\n",
    "    # Load the entire DataFrame from a Parquet file\n",
    "    df = pd.read_parquet(path + fname + '.parquet')\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col[-1] in (\"P\", \"A\"):\n",
    "            df[col] = df[col].astype('float32')\n",
    "\n",
    "        if df[col].dtype.name in ['object', 'string']:\n",
    "            df[col] = df[col].astype(\"string\").astype('category')\n",
    "            current_categories = df[col].cat.categories\n",
    "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
    "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
    "            df[col] = df[col].astype(new_dtype)\n",
    "\n",
    "    return df\n",
    "\n",
    "def depth1_feats(df):\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols.remove('case_id')\n",
    "    numeric_cols.remove('num_group1')\n",
    "    aggfeats = df.groupby('case_id')[numeric_cols].agg('sum').reset_index()\n",
    "\n",
    "    notnum_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "    notnum_cols.append('case_id')\n",
    "    filfeats = df[df['num_group1'] == 0]\n",
    "    filfeats = filfeats.drop('num_group1', axis=1)\n",
    "    filfeats = filfeats.filter(items=notnum_cols)\n",
    "    return pd.merge(filfeats, aggfeats, how='left', on='case_id')\n",
    "\n",
    "def depth2_feats(df):\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols.remove('case_id')\n",
    "    numeric_cols.remove('num_group1')\n",
    "    numeric_cols.remove('num_group2')\n",
    "    aggfeats = df.groupby('case_id')[numeric_cols].agg('sum').reset_index()\n",
    "\n",
    "    notnum_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "    notnum_cols.append('case_id')\n",
    "    df = df[df['num_group1'] == 0]\n",
    "    df = df[df['num_group2'] == 0]\n",
    "    filterdf = df.drop(['num_group1', 'num_group2'], axis=1)\n",
    "    filterdf = filterdf.filter(items=notnum_cols)\n",
    "    return pd.merge(filterdf, aggfeats, how='left', on='case_id') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in base table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/rds/general/user/ms2524/home/amexproject/parquet_files/train/train_'\n",
    "base_cba = pd.read_parquet(path + \"base.parquet\", columns=['case_id','WEEK_NUM','target'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Bureua A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "for id in range(11):\n",
    "    df = filter_df(f'credit_bureau_a_2_{id}')\n",
    "    processed = depth2_feats(df)\n",
    "    all_chunks.append(processed)\n",
    "\n",
    "# Concatenate everything at once\n",
    "tmp = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# Merge with base\n",
    "data_cba = pd.merge(base_cba, tmp, how=\"left\", on=\"case_id\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_appl = pd.merge(\n",
    "    base_cba,\n",
    "    depth2_feats(filter_df('applprev_2')),\n",
    "    how=\"left\",\n",
    "    on=\"case_id\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Bureua A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cba = pd.merge(\n",
    "    data_cba,\n",
    "    depth1_feats(pd.concat([filter_df(f'credit_bureau_a_1_{id}') for id in range(4)])),\n",
    "    how=\"left\",\n",
    "    on=\"case_id\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_appl = pd.merge(\n",
    "    data_appl,\n",
    "    depth1_feats(pd.concat([filter_df(f'applprev_1_{id}') for id in range(2)])),\n",
    "    how=\"left\",\n",
    "    on=\"case_id\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_person_1_train = pd.merge(\n",
    "    base_cba,\n",
    "    depth1_feats(filter_df('person_1')),\n",
    "    how=\"left\",\n",
    "    on=\"case_id\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'case_id', 'annuity_780A', 'credamount_770A', 'disbursedcredamount_1113A', \n",
    "    'eir_270L', 'pmtnum_254L', 'lastst_736L', 'totalsettled_863A', \n",
    "    'numrejects9m_859L', 'currdebt_22A'\n",
    "]\n",
    "\n",
    "data_static_train = pd.merge(\n",
    "    base_cba,\n",
    "    pd.concat([\n",
    "        filter_df(f'static_0_{id}')[columns_to_keep]\n",
    "        for id in range(2)\n",
    "    ], ignore_index=True),\n",
    "    how=\"left\",\n",
    "    on=\"case_id\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_cba.columns:\n",
    "    if col.endswith('D'):\n",
    "        data_cba[col] = pd.to_datetime(data_cba[col], errors='coerce')\n",
    "\n",
    "\n",
    "for col in data_cba.select_dtypes(include=['object']).columns:\n",
    "    data_cba[col] = data_cba[col].astype('category')\n",
    "\n",
    "for col in data_appl.columns:\n",
    "    if col.endswith('D'):\n",
    "        data_appl[col] = pd.to_datetime(data_appl[col], errors='coerce')\n",
    "\n",
    "for col in data_appl.select_dtypes(include=['object']).columns:\n",
    "    data_appl[col] = data_appl[col].astype('category')\n",
    "\n",
    "for col in data_static_train.columns:\n",
    "    if col.endswith('D'):\n",
    "        data_static_train[col] = pd.to_datetime(data_static_train[col], errors='coerce')\n",
    "\n",
    "for col in data_static_train.select_dtypes(include=['object']).columns:\n",
    "    data_static_train[col] = data_static_train[col].astype('category')\n",
    "\n",
    "for col in data_person_1_train.columns:\n",
    "    if col.endswith('D'):\n",
    "        data_person_1_train[col] = pd.to_datetime(data_person_1_train[col], errors='coerce')\n",
    "\n",
    "for col in data_person_1_train.select_dtypes(include=['object']).columns:\n",
    "    data_person_1_train[col] = data_person_1_train[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cb_features = ['case_id', 'WEEK_NUM','pmts_dpd_1073P', 'pmts_dpd_303P', 'dpdmax_139P', 'numberofoverdueinstlmax_1039L', 'numberofoverdueinstls_725L', 'collaterals_typeofguarante_359M', 'classificationofcontr_400M', 'subjectrole_93M', 'target']\n",
    "data1_cba=data_cba[cb_features]\n",
    "data1_cba = data1_cba.copy()\n",
    "data1_cba.replace(['na', 'NaN', '#########'], np.nan, inplace=True)\n",
    "\n",
    "applprev_features = ['case_id', 'WEEK_NUM','maxdpdtolerance_577P', 'cacccardblochreas_147M', 'rejectreason_755M','target']\n",
    "data1_appl=data_appl[applprev_features]\n",
    "data1_appl = data1_appl.copy()\n",
    "data1_appl.replace(['na', 'NaN', '#########'], np.nan, inplace=True)\n",
    "\n",
    "static_features = ['case_id', 'WEEK_NUM','annuity_780A', 'credamount_770A', 'disbursedcredamount_1113A', 'eir_270L', 'pmtnum_254L', 'lastst_736L']\n",
    "data_static_train_1 = data_static_train[static_features]\n",
    "data_static_train_1 = data_static_train_1.copy()\n",
    "data_static_train_1.replace(['na', 'NaN', '#########'], np.nan, inplace=True)\n",
    "\n",
    "person_features = ['case_id', 'WEEK_NUM','personindex_1023L', 'persontype_1072L', 'persontype_792L', 'empladdr_zipcode_114M', 'incometype_1044T', 'safeguarantyflag_411L', 'type_25L','target']\n",
    "data_person_1_train_1 = data_person_1_train[person_features]\n",
    "data_person_1_train_1 = data_person_1_train_1.copy()\n",
    "data_person_1_train_1.replace(['na', 'NaN', '#########'], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Split by WEEK_NUM into equal segments\n",
    "def split_by_week(df, n):\n",
    "    min_week = df[\"WEEK_NUM\"].min()\n",
    "    max_week = df[\"WEEK_NUM\"].max()\n",
    "    bins = np.linspace(min_week, max_week + 1, n + 1, dtype=int)\n",
    "\n",
    "    segments = {}\n",
    "    for i in range(n):\n",
    "        lower = bins[i]\n",
    "        upper = bins[i + 1]\n",
    "        key = f\"split_{i}\"\n",
    "        segments[key] = df[(df[\"WEEK_NUM\"] >= lower) & (df[\"WEEK_NUM\"] < upper)].copy()\n",
    "\n",
    "    return segments\n",
    "\n",
    "def preprocess_split(train_df, test_df):\n",
    "    exclude_columns = {'case_id', 'WEEK_NUM', 'target'}\n",
    "    numcols = []\n",
    "    catcols = []\n",
    "\n",
    "    # Separate numeric and categorical columns\n",
    "    for col in train_df.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(train_df[col]):\n",
    "            numcols.append(col)\n",
    "        elif pd.api.types.is_object_dtype(train_df[col]) or pd.api.types.is_categorical_dtype(train_df[col]):\n",
    "            catcols.append(col)\n",
    "\n",
    "    # Impute numeric columns\n",
    "    for col in numcols:\n",
    "        if train_df[col].isnull().any():\n",
    "            strategy = 'median' if abs(train_df[col].skew()) > 0.5 else 'mean'\n",
    "            imputer = SimpleImputer(strategy=strategy)\n",
    "            train_df[[col]] = imputer.fit_transform(train_df[[col]])\n",
    "            test_df[[col]] = imputer.transform(test_df[[col]])\n",
    "\n",
    "    # Handle categorical columns: fill NA with \"Unknown\"\n",
    "    for col in catcols:\n",
    "        for df in [train_df, test_df]:\n",
    "            if \"Unknown\" not in df[col].cat.categories:\n",
    "                df[col] = df[col].cat.add_categories(\"Unknown\")\n",
    "\n",
    "        # Fill missing values\n",
    "        train_df[col] = train_df[col].fillna(\"Unknown\")\n",
    "        test_df[col] = test_df[col].fillna(\"Unknown\")\n",
    "\n",
    "    # Ordinal encode\n",
    "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    train_df[catcols] = encoder.fit_transform(train_df[catcols])\n",
    "    test_df[catcols] = encoder.transform(test_df[catcols])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def preprocess_all_segments(df, n):\n",
    "    segments = split_by_week(df, n)\n",
    "    base_train = segments[\"split_0\"].copy()\n",
    "    processed_segments = {}\n",
    "\n",
    "    for key, segment_df in segments.items():\n",
    "        train_proc, test_proc = preprocess_split(base_train.copy(), segment_df.copy())\n",
    "        processed_segments[key] = test_proc\n",
    "\n",
    "    return processed_segments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c91997aab5c80800939ef7ebbc3f23cf1e44139d67d3f23153db1a2a22869ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
